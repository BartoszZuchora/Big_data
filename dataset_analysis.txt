{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TMDB 10000 Movies (2022) — EDA + przygotowanie pod klasyfikację\n",
        "\n",
        "Notebook:\n",
        "- ładuje dataset (CSV),\n",
        "- analizuje kolumny (typy, braki, unikalne),\n",
        "- dla kolumn numerycznych podaje: **min, max, std, median** (i dodatkowo mean),\n",
        "- tworzy cechy, buduje prosty model bazowy i rysuje **macierz pomyłek**.\n",
        "\n",
        "## Założenia\n",
        "- Dataset to CSV z Kaggle: anmolkohli1/the-movie-db-10000-movies-2022.\n",
        "- Jeśli Twoja wersja ma inne nazwy kolumn, notebook nadal zadziała, bo analizuje kolumny dynamicznie.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Jeśli uruchamiasz lokalnie: upewnij się, że masz pakiety.\n",
        "# Jeśli na Colab: odkomentuj instalację.\n",
        "\n",
        "# !pip -q install pandas numpy matplotlib seaborn scikit-learn\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "pd.set_option(\"display.width\", 140)\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "RANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Wczytanie danych\n",
        "\n",
        "### Opcja A (najprostsza): plik CSV pobrany ręcznie z Kaggle\n",
        "1. Pobierz dataset z Kaggle.\n",
        "2. Rozpakuj.\n",
        "3. Wskaż `DATA_PATH` do właściwego pliku CSV.\n",
        "\n",
        "### Opcja B: Kaggle API (lokalnie)\n",
        "- Wymaga skonfigurowanego `kaggle.json` (token) w `~/.kaggle/kaggle.json`.\n",
        "- W notebooku poniżej jest przykładowy blok (wyłączony domyślnie).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ustaw ścieżkę do CSV:\n",
        "# Przykład: DATA_PATH = \"./The Movie DB 10000 Movies 2022/tmdb_10000_movies_clean.csv\"\n",
        "\n",
        "DATA_PATH = \"./tmdb_10000_movies_2022.csv\"  # <- ZMIEŃ\n",
        "\n",
        "assert os.path.exists(DATA_PATH), (\n",
        "    \"Nie znaleziono pliku DATA_PATH. Ustaw poprawną ścieżkę do CSV. \"\n",
        "    f\"Podano: {DATA_PATH}\"\n",
        ")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Shape:\", df.shape)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Szybka kontrola jakości\n",
        "- duplikaty\n",
        "- braki danych\n",
        "- typy kolumn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_duplicates = df.duplicated().sum()\n",
        "print(\"Liczba duplikatów wierszy:\", n_duplicates)\n",
        "\n",
        "missing = df.isna().mean().sort_values(ascending=False)\n",
        "display(missing.head(20).to_frame(\"missing_rate\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Automatyczna analiza każdej kolumny\n",
        "\n",
        "Dla każdej kolumny raportujemy:\n",
        "- dtype\n",
        "- liczba braków i % braków\n",
        "- liczba unikalnych wartości\n",
        "- jeśli numeryczna: min/max/std/median (+ mean)\n",
        "\n",
        "Uwaga: jeśli jakieś kolumny są liczbami zapisanymi jako tekst (np. z przecinkami), spróbujemy je bezpiecznie skonwertować.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def try_parse_numeric(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"Próba konwersji string->numeric (usuwa $, przecinki, spacje).\"\"\"\n",
        "    if series.dtype != \"object\":\n",
        "        return series\n",
        "\n",
        "    s = series.astype(str)\n",
        "    s = s.replace({\"None\": np.nan, \"nan\": np.nan, \"NaN\": np.nan})\n",
        "    s = s.str.replace(r\"[\\$,]\", \"\", regex=True).str.strip()\n",
        "\n",
        "    numeric = pd.to_numeric(s, errors=\"coerce\")\n",
        "    # jeśli konwersja ma sens (wystarczająco dużo wartości przeszło), zwróć numeric\n",
        "    non_na_original = series.notna().sum()\n",
        "    non_na_numeric = numeric.notna().sum()\n",
        "    if non_na_original == 0:\n",
        "        return series\n",
        "    if non_na_numeric / non_na_original >= 0.85:\n",
        "        return numeric\n",
        "    return series\n",
        "\n",
        "\n",
        "df2 = df.copy()\n",
        "for c in df2.columns:\n",
        "    df2[c] = try_parse_numeric(df2[c])\n",
        "\n",
        "def column_report(frame: pd.DataFrame) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for col in frame.columns:\n",
        "        s = frame[col]\n",
        "        n = len(s)\n",
        "        n_missing = int(s.isna().sum())\n",
        "        missing_rate = n_missing / n if n else np.nan\n",
        "        nunique = int(s.nunique(dropna=True))\n",
        "        dtype = str(s.dtype)\n",
        "\n",
        "        stats = {\n",
        "            \"min\": np.nan,\n",
        "            \"max\": np.nan,\n",
        "            \"std\": np.nan,\n",
        "            \"median\": np.nan,\n",
        "            \"mean\": np.nan,\n",
        "        }\n",
        "        if pd.api.types.is_numeric_dtype(s):\n",
        "            stats[\"min\"] = float(np.nanmin(s.values)) if s.notna().any() else np.nan\n",
        "            stats[\"max\"] = float(np.nanmax(s.values)) if s.notna().any() else np.nan\n",
        "            stats[\"std\"] = float(np.nanstd(s.values, ddof=1)) if s.notna().sum() > 1 else np.nan\n",
        "            stats[\"median\"] = float(np.nanmedian(s.values)) if s.notna().any() else np.nan\n",
        "            stats[\"mean\"] = float(np.nanmean(s.values)) if s.notna().any() else np.nan\n",
        "\n",
        "        rows.append(\n",
        "            {\n",
        "                \"column\": col,\n",
        "                \"dtype\": dtype,\n",
        "                \"missing_count\": n_missing,\n",
        "                \"missing_rate\": missing_rate,\n",
        "                \"nunique\": nunique,\n",
        "                **stats,\n",
        "            }\n",
        "        )\n",
        "    return (\n",
        "        pd.DataFrame(rows)\n",
        "        .sort_values([\"missing_rate\", \"nunique\"], ascending=[False, False])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "\n",
        "report = column_report(df2)\n",
        "display(report)\n",
        "\n",
        "# Zapis raportu do CSV obok danych\n",
        "out_report_path = os.path.join(os.path.dirname(DATA_PATH), \"column_report.csv\")\n",
        "report.to_csv(out_report_path, index=False)\n",
        "print(\"Zapisano raport:\", out_report_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Wizualizacje: rozkłady, korelacje\n",
        "\n",
        "### 4.1 Rozkłady wybranych kolumn numerycznych\n",
        "Automatycznie wybieramy kilka numerycznych kolumn o największej wariancji.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cols = [c for c in df2.columns if pd.api.types.is_numeric_dtype(df2[c])]\n",
        "print(\"Liczba kolumn numerycznych:\", len(num_cols))\n",
        "\n",
        "if len(num_cols) > 0:\n",
        "    variances = df2[num_cols].var(numeric_only=True).sort_values(ascending=False)\n",
        "    top = variances.head(8).index.tolist()\n",
        "    display(variances.head(15).to_frame(\"variance\"))\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(18, 8))\n",
        "    axes = axes.flatten()\n",
        "    for ax, col in zip(axes, top):\n",
        "        sns.histplot(df2[col], kde=False, ax=ax, bins=40)\n",
        "        ax.set_title(col)\n",
        "    for ax in axes[len(top) :]:\n",
        "        ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "else:\n",
        "    print(\"Brak kolumn numerycznych do pokazania.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Mapa korelacji (numeryczne)\n",
        "Uwaga: korelacje mają sens tylko dla sensownie numerycznych danych.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(num_cols) >= 2:\n",
        "    corr = df2[num_cols].corr(numeric_only=True)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
        "    plt.title(\"Korelacje (kolumny numeryczne)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Za mało kolumn numerycznych do korelacji.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Przygotowanie pod klasyfikację\n",
        "\n",
        "W tym datasetcie zwykle nie ma gotowej etykiety klasy (target). Trzeba ją zdefiniować.\n",
        "\n",
        "Poniżej tworzę przykładowy target binarny `target_hit`:\n",
        "- jeśli istnieje kolumna `revenue`: film jest \"hitem\" gdy `revenue` >= 75 percentyla.\n",
        "- jeśli `revenue` nie ma, ale jest `vote_average`: \"hit\" gdy `vote_average` >= 75 percentyla.\n",
        "- jeśli żadnej nie ma, notebook poprosi o ręczny wybór.\n",
        "\n",
        "Ważne: to tylko baseline — docelowo wybierz target zgodny z Twoim zadaniem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pick_target_column(frame: pd.DataFrame) -> str:\n",
        "    candidates = [\"revenue\", \"vote_average\", \"popularity\", \"vote_count\"]\n",
        "    for c in candidates:\n",
        "        if c in frame.columns and pd.api.types.is_numeric_dtype(frame[c]):\n",
        "            if frame[c].notna().sum() > 50:\n",
        "                return c\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "target_source = pick_target_column(df2)\n",
        "print(\"Wybrana kolumna bazowa do targetu:\", target_source or \"BRAK\")\n",
        "\n",
        "if not target_source:\n",
        "    raise ValueError(\n",
        "        \"Nie znaleziono sensownej kolumny do zbudowania targetu. \"\n",
        "        \"Podaj ręcznie nazwę kolumny numerycznej (np. revenue/vote_average).\"\n",
        "    )\n",
        "\n",
        "q = df2[target_source].quantile(0.75)\n",
        "df2[\"target_hit\"] = (df2[target_source] >= q).astype(int)\n",
        "print(f\"Próg (75 percentyl) dla {target_source}: {q}\")\n",
        "df2[\"target_hit\"].value_counts(normalize=True).rename(\"share\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Feature engineering (lekki, bezpieczny)\n",
        "\n",
        "- Usuwamy kolumny, które są oczywistym przeciekiem targetu (np. jeśli target z `revenue`, to `revenue` usuwamy z cech).\n",
        "- Wykrywamy kolumny tekstowe z bardzo wysoką krotnością (np. `overview`) i domyślnie je pomijamy (baseline).\n",
        "\n",
        "Jeżeli chcesz model z NLP na `overview`/`tagline`, to zrobimy osobny pipeline (TF-IDF). Tu skupiamy się na klasycznym baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_col = \"target_hit\"\n",
        "\n",
        "# Potencjalne kolumny ID / przecieki / zbyt opisowe\n",
        "drop_cols = {target_col}\n",
        "drop_cols.add(target_source)  # usuń źródło targetu by uniknąć leakage\n",
        "\n",
        "# Typowe kolumny identyfikatorów (jeśli istnieją)\n",
        "for c in [\"id\", \"imdb_id\", \"tmdb_id\", \"title\", \"original_title\"]:\n",
        "    if c in df2.columns:\n",
        "        # title czasem daje sygnał, ale zwykle słabo uogólnia; do baseline drop\n",
        "        drop_cols.add(c)\n",
        "\n",
        "# Heurystyka: bardzo długi tekst -> drop w baseline\n",
        "text_like = []\n",
        "for c in df2.columns:\n",
        "    if df2[c].dtype == \"object\":\n",
        "        avg_len = df2[c].dropna().astype(str).str.len().mean() if df2[c].notna().any() else 0\n",
        "        if avg_len and avg_len > 80:\n",
        "            text_like.append(c)\n",
        "drop_cols.update(text_like)\n",
        "\n",
        "print(\"Kolumny odrzucone (baseline):\")\n",
        "print(sorted(drop_cols))\n",
        "\n",
        "X = df2.drop(columns=[c for c in drop_cols if c in df2.columns])\n",
        "y = df2[target_col].astype(int)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y distribution:\")\n",
        "display(y.value_counts().to_frame(\"count\"))\n",
        "\n",
        "num_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
        "cat_features = [c for c in X.columns if X[c].dtype == \"object\"]\n",
        "\n",
        "print(\"Num features:\", len(num_features))\n",
        "print(\"Cat features:\", len(cat_features))\n",
        "display(pd.DataFrame({\"num\": num_features}).head(20))\n",
        "display(pd.DataFrame({\"cat\": cat_features}).head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Model bazowy + macierz pomyłek\n",
        "\n",
        "- Pipeline:\n",
        "  - num: imputacja medianą\n",
        "  - cat: imputacja \"missing\" + OneHotEncoder\n",
        "  - model: LogisticRegression\n",
        "\n",
        "Wynik: raport klasyfikacji + macierz pomyłek.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "numeric_transformer = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\n",
        "            \"onehot\",\n",
        "            OneHotEncoder(handle_unknown=\"ignore\", min_frequency=10),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, num_features),\n",
        "        (\"cat\", categorical_transformer, cat_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "clf = LogisticRegression(max_iter=2000, n_jobs=None)\n",
        "\n",
        "model = Pipeline(steps=[(\"preprocess\", preprocess), (\"clf\", clf)])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(values_format=\"d\")\n",
        "plt.title(\"Macierz pomyłek\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ROC-AUC (jeśli możliwe)\n",
        "if hasattr(model.named_steps[\"clf\"], \"predict_proba\"):\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    print(\"ROC-AUC:\", round(auc, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Wnioski i następne kroki (pod Twój docelowy model)\n",
        "\n",
        "Co warto zrobić dalej:\n",
        "1. Zdefiniować właściwy target (np. gatunek, sukces finansowy, ocena powyżej progu).\n",
        "2. Zdecydować które kolumny są \"leakage\" (np. `revenue` jeśli target to hit po revenue).\n",
        "3. Dodać NLP dla `overview`/`tagline` (TF-IDF lub model transformer).\n",
        "4. Użyć mocniejszych modeli (LightGBM/CatBoost/XGBoost) i walidacji krzyżowej.\n",
        "5. Zrobić tuning progu klasyfikacji i analizę kalibracji.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}